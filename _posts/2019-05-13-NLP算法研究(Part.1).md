---
layout:     post
title:      NLPç®—æ³•ç ”ç©¶ï¼Œä»å…¥é—¨åˆ°å…¥åŸ

subtitle:   ç¬¬ä¸€é˜¶æ®µ
date:       2019-05-23
author:     Laqudee.Zhao
header-img: img/post-bg-debug.png
catalog: true
tags:
    - NLP
    - ML
---

# å‰è¨€
NLPï¼Œå³è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ä¸€ç»„ç”¨äºå¤„ç†æ–‡æœ¬é—®é¢˜çš„æŠ€æœ¯ã€‚è¿™ç¯‡åšå®¢æºè‡ª[Kaggle NLPæ•™ç¨‹](https://www.kaggle.com/c/word2vec-nlp-tutorial)ä»¥åŠ[ç®€ä¹¦ä¸Šå¯¹Kaggleçš„ç¿»è¯‘ï¼Œä½œè€…ï¼šApacheCN_é£é¾™](https://www.jianshu.com/p/d2e6568250bd)ï¼Œå¯¹æ­¤è¡¨ç¤ºæ„Ÿè°¢ï¼  
ä»æ•°æ®åŠ è½½å’Œæ¸…ç†IMDBç”µå½±è¯„è®ºèµ·æ­¥ï¼Œç„¶ååº”ç”¨ä¸€ä¸ªç®€å•çš„è¯è¢‹æ¨¡å‹ï¼Œæ¥è·å–ä»¤äººæƒŠè®¶çš„å‡†ç¡®é¢„æµ‹ã€‚  

æŠ€æœ¯æ ˆï¼š
- python
- NLPç®—æ³•åŒ…(nltk)
- è®¡ç®—ã€ç»˜å›¾ç­‰å…¶ä»–MLåŒ…

ä¸€ä¸ªç”¨Pythonå†™çš„å…³äºNLPå’Œæ–‡æœ¬å¤„ç†çš„[æ•´éƒ¨ä¹¦](http://www.nltk.org/book/)

ç¬¬ä¸€éƒ¨åˆ†ä»£ç ä¸‹è½½[ç‚¹å‡»ä¸‹è½½](https://github.com/wendykan/DeepLearningMovies/blob/master/BagOfWords.py)

# è¯»å–æ•°æ®
ç¬¬ä¸€ä¸ªæ–‡ä»¶æ˜¯```py unlabeledTrainData ```ï¼Œå…¶ä¸­åŒ…å«25000ä¸ªIMDBç”µå½±è¯„ä»·ï¼Œæ¯ä¸ªè¯„ä»·éƒ½å¸¦æœ‰æ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿæ ‡ç­¾ã€‚

æ¥ç€ï¼Œå°†åˆ¶è¡¨ç¬¦åˆ†éš”æ–‡ä»¶è¯»å…¥Pythonï¼Œä½¿ç”¨pandasä¸­çš„read_csvå‡½æ•°ï¼š   

```python
import pandas as pd
train = pd.read_csv("labeledTrainData.tsv", header=0, delimiter="\t", quting=3)
# header=0è¡¨ç¤ºæ–‡ä»¶çš„ç¬¬ä¸€è¡ŒåŒ…å«åˆ—åï¼Œdelimiter=\tè¡¨ç¤ºå­—æ®µç”±åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œquting=3è®©Pythonå¿½ç•¥åŒå¼•å·
```

ç¡®ä¿è¯»å–25000è¡Œå’Œ3åˆ—ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
train.shape
(25000, 3)

train.columns.values
array([id, sentiment, review], dtype=object)
# è¯´æ˜å·²ç»è¯»å–åˆ°è®­ç»ƒé›†

# æŸ¥çœ‹å‡ æ¡è¯„è®º
print train["review"][0]
```

æ–‡æœ¬å¯èƒ½å­˜åœ¨HTMLæ ‡ç­¾ï¼Œè¦ä½¿ç”¨å·¥å…·å°†ä¹‹åˆ é™¤

# æ•°æ®æ¸…ç†å’Œæ–‡æœ¬é¢„å¤„ç†
åˆ é™¤HTMLæ ‡è®°ï¼šBeautifulSoupåŒ…   

å¦‚æœæ²¡æœ‰å®‰è£…è¿™ä¸ªåŒ…ï¼Œåˆ™è¦å…ˆå®‰è£…ï¼ç„¶åä»pythonä¸­åŠ è½½åŒ…å¹¶ä½¿ç”¨å®ƒä»è¯„è®ºä¸­æå–æ–‡æœ¬ï¼š

```python
from bs4 import BeautifulSoup
# Initialize the BeautifulSoup object on a single movie review     
example1 = BeautifulSoup(train["review"][0])  

# Print the raw review and then the output of get_text(), for 
# comparison
print train["review"][0]
print example1.get_text() # get_text()ä¼šæä¾›ä¸å¸¦æ ‡ç­¾çš„è¯„è®ºæ–‡æœ¬
```

## å¤„ç†æ ‡ç‚¹ï¼Œæ•°å­—å’Œåœæ­¢ç¬¦ï¼šNLTKå’Œæ­£åˆ™è¡¨è¾¾å¼
- åœ¨è€ƒè™‘å¦‚ä½•æ¸…ç†æ–‡æœ¬æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘æˆ‘ä»¬è¯•å›¾è§£å†³çš„æ•°æ®é—®é¢˜ã€‚å¯¹äºè®¸å¤šé—®é¢˜ï¼Œåˆ é™¤æ ‡ç‚¹ç¬¦å·æ˜¯æœ‰æ„ä¹‰çš„ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ­£åœ¨è§£å†³æƒ…æ„Ÿåˆ†æé—®é¢˜ï¼Œå¹¶ä¸”æœ‰å¯èƒ½"!!!"æˆ–è€…":-("å¯ä»¥å¸¦æœ‰æƒ…æ„Ÿï¼Œåº”è¯¥è¢«è§†ä¸ºå•è¯ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å®Œå…¨åˆ é™¤äº†æ ‡ç‚¹ç¬¦å·ï¼Œä½†è¿™æ˜¯ä½ å¯ä»¥è‡ªå·±ç©çš„ä¸œè¥¿ã€‚
  
- ä¸ä¹‹ç›¸ä¼¼ï¼Œåœ¨æœ¬æ•™ç¨‹ä¸­æˆ‘ä»¬å°†åˆ é™¤æ•°å­—ï¼Œä½†è¿˜æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥å¤„ç†å®ƒä»¬ï¼Œè¿™äº›æ–¹æ³•åŒæ ·æœ‰æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è§†ä¸ºå•è¯ï¼Œæˆ–è€…ä½¿ç”¨å ä½ç¬¦å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚"NUM"ï¼‰æ›¿æ¢å®ƒä»¬ã€‚

- è¦åˆ é™¤æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåŒ…æ¥å¤„ç†æ­£åˆ™è¡¨è¾¾å¼ï¼Œç§°ä¸ºreã€‚Python å†…ç½®äº†è¯¥è½¯ä»¶åŒ…ï¼›æ— éœ€å®‰è£…ä»»ä½•ä¸œè¥¿ã€‚å¯¹äºæ­£åˆ™è¡¨è¾¾å¼å¦‚ä½•å·¥ä½œçš„è¯¦ç»†è¯´æ˜ï¼Œè¯·å‚é˜…[åŒ…æ–‡æ¡£](https://docs.python.org/2/library/re.html#)ã€‚ç°åœ¨ï¼Œå°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š   

```python
import re
# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰§è¡ŒæŸ¥æ‰¾å’Œæ›¿æ¢
letters_only = re.sub("[^a-zA-Z]", " ", example1.get_text())

print letters_only
```

æ¥ç€ï¼Œå°†è¯„è®ºè½¬æ¢ä¸ºå°å†™å¹¶å°†å®ƒä»¬åˆ†æˆå•è¯ï¼ˆNLPæœ¯è¯­ä¸­ç§°ä¸ºâ€œåˆ†è¯â€ï¼‰ï¼š  

```python
lower_case = letters_only.lower()        # è½¬æ¢ä¸ºå°å†™
words = lower_case.split()               # åˆ†å‰²ä¸ºå•è¯
```

æœ€åï¼Œå¤„ç†åœæ­¢è¯ï¼ˆè¯¸å¦‚ï¼ša, and, is, theç­‰ï¼‰ã€‚PythonåŒ…ä¸­å†…ç½®äº†åœæ­¢è¯åˆ—è¡¨ã€‚ä»NLTKä¸­å¯¼å…¥åœæ­¢è¯åˆ—è¡¨ï¼š

```python
# å¦‚æœæ²¡æœ‰å®‰è£…nltkï¼Œè¯·å…ˆå®‰è£…
import nltk
nltk.download() # ä¸‹è½½æ–‡æœ¬æ•°æ®é›†ï¼ŒåŒ…å«åœæ­¢è¯

# ä½¿ç”¨nltkè·å–åœæ­¢è¯åˆ—è¡¨
from nltk.corpus import stopwords
print stopwords.words("english")

# ä»â€œwordsâ€ä¸­ç§»é™¤åœæ­¢è¯
words = [w for w in words if not w stopwords.words("english")]
print words

# å®Œæˆæ‰€æœ‰æ­¥éª¤åï¼Œå¾—åˆ°
[u'stuff', u'going', u'moment', u'mj', u've', u'started', u'listening', u'music', u'watching', u'odd', u'documentary', u'watched', u'wiz', u'watched', u'moonwalker', u'maybe', u'want', u'get', u'certain', u'insight', u'guy', u'thought', u'really', u'cool', u'eighties', u'maybe', u'make', u'mind', u'whether', u'guilty', u'innocent', u'moonwalker', u'part', u'biography', u'part', u'feature', u'film', u'remember', u'going', u'see', u'cinema', u'originally', u'released', u'subtle', u'messages', u'mj', u'feeling', u'towards', u'press', u'also', u'obvious', u'message', u'drugs', u'bad', u'm', u'kay',.....]
```

æˆ‘ä»¬è¿˜å¯ä»¥å¯¹æ•°æ®åšå¾ˆå¤šå…¶ä»–æ“ä½œï¼Œå¦‚porter stemming(è¯å¹²æå–)å’ŒLemmatizing(è¯å½¢è¿˜åŸ)ç­‰ã€‚éƒ½åœ¨NLTKåŒ…ä¸­æä¾›ã€‚

## ä¸ºäº†ä½¿ä»£ç å¯ä»¥é‡ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªå¯ä»¥é‡å¤ä½¿ç”¨çš„å‡½æ•°

```python
def review_to_words(raw_review):
    # å°†åŸå§‹è¯„è®ºè½¬æ¢ä¸ºå•è¯å­—ç¬¦ä¸²çš„å‡½æ•°
    # è¾“å…¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼ˆåŸå§‹ç”µå½±è¯„è®ºï¼‰ï¼Œ
    # è¾“å‡ºæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼ˆé¢„å¤„ç†è¿‡çš„ç”µå½±è¯„è®ºï¼‰
    # 1. ç§»é™¤ HTML
    review_text = BeautifulSoup(raw_review).get_text() 
    #
    # 2. ç§»é™¤éå­—æ¯        
    letters_only = re.sub("[^a-zA-Z]", " ", review_text) 
    #
    # 3. è½¬æ¢ä¸ºå°å†™ï¼Œåˆ†æˆå•ä¸ªå•è¯
    words = letters_only.lower().split()                             
    #
    # 4. åœ¨Pythonä¸­ï¼Œæœç´¢é›†åˆæ¯”æœç´¢åˆ—è¡¨å¿«å¾—å¤šï¼Œ
    #    æ‰€ä»¥å°†åœæ­¢è¯è½¬æ¢ä¸ºä¸€ä¸ªé›†åˆ
    stops = set(stopwords.words("english"))                  
    # 
    # 5. åˆ é™¤åœæ­¢è¯
    meaningful_words = [w for w in words if not w in stops]   
    #
    # 6. å°†å•è¯è¿æ¥æˆç”±ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œ
    #    å¹¶è¿”å›ç»“æœã€‚
    return( " ".join( meaningful_words ))

```

è¿™é‡Œæœ‰ä¸¤ä¸ªæ–°å…ƒç´ ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°†åœæ­¢è¯åˆ—è¡¨è½¬æ¢ä¸ºä¸åŒçš„æ•°æ®ç±»å‹ï¼Œå³é›†åˆã€‚ è¿™æ˜¯ä¸ºäº†é€Ÿåº¦ï¼›å› ä¸ºæˆ‘ä»¬å°†è°ƒç”¨è¿™ä¸ªå‡½æ•°æ•°ä¸‡æ¬¡ï¼Œæ‰€ä»¥å®ƒéœ€è¦å¾ˆå¿«ï¼Œè€Œ Python ä¸­çš„æœç´¢é›†åˆæ¯”æœç´¢åˆ—è¡¨è¦å¿«å¾—å¤šã€‚

# ä»è¯è¢‹åˆ›å»ºç‰¹å¾ï¼ˆä½¿ç”¨sklearnï¼‰
ç°åœ¨æˆ‘ä»¬å·²ç»æ•´ç†äº†æˆ‘ä»¬çš„è®­ç»ƒè¯„è®ºï¼Œæˆ‘ä»¬å¦‚ä½•å°†å®ƒä»¬è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ çš„æŸç§æ•°å­—è¡¨ç¤ºï¼Ÿä¸€ç§å¸¸è§çš„æ–¹æ³•å«åšè¯è¢‹ã€‚è¯è¢‹æ¨¡å‹ä»æ‰€æœ‰æ–‡æ¡£ä¸­å­¦ä¹ è¯æ±‡è¡¨ï¼Œç„¶åé€šè¿‡è®¡ç®—æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œå»ºæ¨¡ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹ä¸¤å¥è¯ï¼š

å¥å­1: "The cat sat on the hat"

å¥å­2: "The dog ate the cat and the hat"

ä»è¿™ä¸¤ä¸ªå¥å­ä¸­ï¼Œæˆ‘ä»¬çš„è¯æ±‡å¦‚ä¸‹ï¼š

```python
{the, cat, sat, on, hat, dog, ate, and}
```

ä¸ºäº†å¾—åˆ°æˆ‘ä»¬çš„è¯è¢‹ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªå•è¯å‡ºç°åœ¨æ¯ä¸ªå¥å­ä¸­çš„æ¬¡æ•°ã€‚åœ¨å¥å­ 1 ä¸­ï¼Œâ€œtheâ€å‡ºç°ä¸¤æ¬¡ï¼Œâ€œcatâ€ï¼Œâ€œsatâ€ï¼Œâ€œonâ€å’Œâ€œhatâ€æ¯æ¬¡å‡ºç°ä¸€æ¬¡ï¼Œå› æ­¤å¥å­ 1 çš„ç‰¹å¾å‘é‡æ˜¯ï¼š

```python
{the, cat, sat, on, hat, dog, ate, and}

#å¥å­1ç‰¹å¾:
{2, 1, 1, 1, 1, 0, 0, 0}
#å¥å­2ç‰¹å¾:
{ 3, 1, 0, 0, 1, 1, 1, 1}
```

åœ¨ IMDB æ•°æ®ä¸­ï¼Œæˆ‘ä»¬æœ‰å¤§é‡çš„è¯„è®ºï¼Œè¿™å°†ä¸ºæˆ‘ä»¬æä¾›å¤§é‡çš„è¯æ±‡ã€‚è¦é™åˆ¶ç‰¹å¾å‘é‡çš„å¤§å°ï¼Œæˆ‘ä»¬åº”è¯¥é€‰æ‹©æœ€å¤§è¯æ±‡é‡ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨ 5000 ä¸ªæœ€å¸¸ç”¨çš„å•è¯ï¼ˆè®°ä½å·²ç»åˆ é™¤äº†åœæ­¢è¯ï¼‰ã€‚

ä½¿ç”¨scikit-learnä¸­çš„feature_extractionæ¨¡å—æ¥åˆ›å»ºè¯è¢‹ç‰¹å¾ã€‚

```python
print "Creating the bag of words...\n"
from sklearn.feature_extraction.text import CountVectorizer

# åˆå§‹åŒ– "CountVectorizer" å¯¹è±¡ï¼Œ
# è¿™æ˜¯ scikit-learn çš„ä¸€ä¸ªè¯è¢‹å·¥å…·ã€‚
vectorizer = CountVectorizer(analyzer = "word",   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = None,   \
                             max_features = 5000) 

# fit_transform() æœ‰ä¸¤ä¸ªåŠŸèƒ½ï¼š
# é¦–å…ˆï¼Œå®ƒæ‹Ÿåˆæ¨¡å‹å¹¶å­¦ä¹ è¯æ±‡ï¼›
# ç¬¬äºŒï¼Œå®ƒå°†æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºç‰¹å¾å‘é‡ã€‚
# fit_transform çš„è¾“å…¥åº”è¯¥æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
train_data_features = vectorizer.fit_transform(clean_train_reviews)

# Numpy æ•°ç»„å¾ˆå®¹æ˜“ä½¿ç”¨ï¼Œå› æ­¤å°†ç»“æœè½¬æ¢ä¸ºæ•°ç»„
train_data_features = train_data_features.toarray()

# æŸ¥çœ‹è®­ç»ƒæ•°æ®æ•°ç»„ç°åœ¨çš„æ ·å­ï¼š
print train_data_features.shape
(25000, 5000) # å®ƒæœ‰ 25,000 è¡Œå’Œ 5,000 ä¸ªç‰¹å¾ï¼ˆæ¯ä¸ªè¯æ±‡ä¸€ä¸ªï¼‰ã€‚

# è¯è¢‹æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œçœ‹çœ‹è¯æ±‡è¡¨ï¼š
vocab = vectorizer.get_feature_names()
print vocab

```

# éšæœºæ£®æ—
åˆ°äº†è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰è¯è¢‹çš„æ•°å­—è®­ç»ƒç‰¹å¾å’Œæ¯ä¸ªç‰¹å¾å‘é‡çš„åŸå§‹æƒ…æ„Ÿæ ‡ç­¾ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬åšä¸€äº›ç›‘ç£å­¦ä¹ ï¼

```python
print "Training the random forest..."
from sklearn.ensemble import RandomForestClassifier

# ä½¿ç”¨ 100 æ£µæ ‘åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨
forest = RandomForestClassifier(n_estimators = 100) 

# ä½¿ç”¨è¯è¢‹ä½œä¸ºç‰¹å¾å¹¶å°†æƒ…æ„Ÿæ ‡ç­¾ä½œä¸ºå“åº”å˜é‡ï¼Œä½¿æ£®æ—æ‹Ÿåˆè®­ç»ƒé›†
# è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ¥è¿è¡Œ
forest = forest.fit( train_data_features, train["sentiment"] )

```

# åˆ›å»ºæäº¤
å‰©ä¸‹çš„å°±æ˜¯åœ¨æˆ‘ä»¬çš„æµ‹è¯•é›†ä¸Šè¿è¡Œè®­ç»ƒå¥½çš„éšæœºæ£®æ—å¹¶åˆ›å»ºä¸€ä¸ªæäº¤æ–‡ä»¶ã€‚ å¦‚æœä½ è¿˜æ²¡æœ‰è¿™æ ·åšï¼Œè¯·ä»â€œæ•°æ®â€é¡µé¢ä¸‹è½½testData.tsvã€‚ æ­¤æ–‡ä»¶åŒ…å«å¦å¤– 25,000 æ¡è¯„è®ºå’Œæ ‡ç­¾ï¼›æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾ã€‚

è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨è¯è¢‹ä½œä¸ºæµ‹è¯•é›†æ—¶ï¼Œæˆ‘ä»¬åªè°ƒç”¨transformï¼Œè€Œä¸æ˜¯åƒè®­ç»ƒé›†é‚£æ ·è°ƒç”¨fit_transformã€‚ åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œä½ ä¸åº”è¯¥ä½¿ç”¨æµ‹è¯•é›†æ¥æ‹Ÿåˆä½ çš„æ¨¡å‹ï¼Œå¦åˆ™ä½ å°†é¢ä¸´è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ å‡ºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬å°†æµ‹è¯•é›†ä¿æŒåœ¨ç¦æ­¢çŠ¶æ€ï¼Œç›´åˆ°æˆ‘ä»¬å‡†å¤‡å¥½è¿›è¡Œé¢„æµ‹ã€‚

```python
# è¯»å–æµ‹è¯•æ•°æ®
test = pd.read_csv("testData.tsv", header=0, delimiter="\t", \
                   quoting=3 )

# éªŒè¯æœ‰ 25,000 è¡Œå’Œ 2 åˆ—
print test.shape

# åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨å¹¶é€ä¸ªé™„åŠ å¹²å‡€çš„è¯„è®º
num_reviews = len(test["review"])
clean_test_reviews = [] 

print "Cleaning and parsing the test set movie reviews...\n"
for i in xrange(0,num_reviews):
    if( (i+1) % 1000 == 0 ):
        print "Review %d of %d\n" % (i+1, num_reviews)
    clean_review = review_to_words( test["review"][i] )
    clean_test_reviews.append( clean_review )

# è·å–æµ‹è¯•é›†çš„è¯è¢‹ï¼Œå¹¶è½¬æ¢ä¸º numpy æ•°ç»„
test_data_features = vectorizer.transform(clean_test_reviews)
test_data_features = test_data_features.toarray()

# ä½¿ç”¨éšæœºæ£®æ—è¿›è¡Œæƒ…æ„Ÿæ ‡ç­¾é¢„æµ‹
result = forest.predict(test_data_features)

# å°†ç»“æœå¤åˆ¶åˆ°å¸¦æœ‰ "id" åˆ—å’Œ "sentiment" åˆ—çš„ pandas dataframe
output = pd.DataFrame( data={"id":test["id"], "sentiment":result} )

# ä½¿ç”¨ pandas ç¼–å†™é€—å·åˆ†éš”çš„è¾“å‡ºæ–‡ä»¶
output.to_csv( "Bag_of_Words_model.csv", index=False, quoting=3 )
```

æ­å–œï¼Œä½ å·²å‡†å¤‡å¥½ç¬¬ä¸€æ¬¡æäº¤ï¼ å°è¯•ä¸åŒçš„äº‹æƒ…ï¼Œçœ‹çœ‹ä½ çš„ç»“æœå¦‚ä½•å˜åŒ–ã€‚ ä½ å¯ä»¥ä»¥ä¸åŒæ–¹å¼æ¸…ç†è¯„è®ºï¼Œä¸ºè¯è¢‹è¡¨ç¤ºé€‰æ‹©ä¸åŒæ•°é‡çš„è¯æ±‡è¡¨å•è¯ï¼Œå°è¯• Porter Stemmingï¼Œä¸åŒçš„åˆ†ç±»å™¨æˆ–ä»»ä½•å…¶ä»–çš„ä¸œè¥¿ã€‚ è¦åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¯•ç”¨ä½ çš„ NLP æ‹›å¼ï¼Œä½ è¿˜å¯ä»¥å‚åŠ æˆ‘ä»¬çš„çƒ‚ç•ªèŒ„æ¯”èµ›ã€‚ æˆ–è€…ï¼Œå¦‚æœä½ ä¸ºå®Œå…¨ä¸åŒçš„ä¸œè¥¿åšå¥½äº†å‡†å¤‡ï¼Œè¯·è®¿é—®æ·±åº¦å­¦ä¹ å’Œè¯å‘é‡é¡µé¢ã€‚


> å­¦ä¹ æº[ç®€ä¹¦](https://www.jianshu.com/p/d2e6568250bd)ï¼Œæå‡ºæ„Ÿè°¢ï¼

# ä½¿ç”¨NLPåˆ›å»ºTwitterè¯„ä»·åˆ†ç±»å™¨çš„æƒ³æ³•
- å–ä»£äººå·¥æ‰“æ ‡æ³¨çš„è¿‡ç¨‹ï¼Œæé«˜æ•°æ®åˆ†æã€æŒ–æ˜æ•ˆç‡
## æ¦‚è¿°
å‚ç…§ä¹‹å‰çš„è‡ªç„¶è¯­è¨€ç®—æ³•ç ”ç©¶åšå®¢ï¼Œå¾—çŸ¥å¤„ç†æ¨æ–‡æ˜¯å¯è¡Œçš„ï¼Œé‡ç‚¹æ˜¯å¦‚ä½•åˆç†çš„åˆ†å‡ºæ¨æ–‡ä¸­çš„æ„Ÿæƒ…è‰²å½©å’Œä½¿ç”¨å“ªç§åˆ†ç±»ç®—æ³•æ¥å¤„ç†ï¼è‹¥ç ”ç©¶å‡ºç»“æœå¯ç”¨äºå°è§„æ¨¡çš„èˆ†æƒ…åˆ†ææœ‰ä¸€å®šçš„ä»·å€¼ã€‚

è¿™ä¸ªæƒ³æ³•äº§ç”Ÿäºå—äº¬é¢è¯•ã€5æœˆ13å·ã€‘ä¹‹åï¼ç›®å‰è§‰å¾—å¯è¡Œï¼åšè¿™æ–¹é¢å·¥ä½œçš„ç ”ç©¶äººå‘˜ä¹Ÿå¾ˆå¤šï¼Œæˆ‘åªæ˜¯ä¸ªåˆå­¦è€…ï¼ğŸ˜Š

ä¸€å®šä¼šä½¿ç”¨åˆ°æœºå™¨å­¦ä¹ åº“æˆ–åŒ…ï¼

å·¥å…·åˆå®šï¼šJava + NLP + classification-Alg æˆ–è€… Python + NLP + classification-Alg

> æœ¬æ–‡é¦–æ¬¡å‘å¸ƒäº [YunLongBlog](http://ZhaoYLong.github.io), ä½œè€… [Laqudee](http://www.twtter.com/Laqudee1) ,è½¬è½½è¯·ä¿ç•™åŸæ–‡é“¾æ¥.
