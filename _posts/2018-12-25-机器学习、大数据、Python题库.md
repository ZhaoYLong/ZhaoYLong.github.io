---
layout:     post
title:      机器学习、大数据、Python试题库

subtitle:   
date:       2018-12-25
author:     Yunlong
header-img: img/post-bg-debug.png
catalog:    true
tag:
    - ML
---

### （Python-ML）招聘基本要求：
* 1、数据库Mysql及web框架Django
* 2、Python数据分析
* 3、Python的科学计算库、机器学习库等
* 4、基于Python的数据结构和算法研究
* 5、各种机器学习算法

### 应对策略：（一切都是基于Python）
* 1、刷BAT的大数据（机器学习、深度学习）笔/面试题
* 2、加深对数据处理分析的流程熟悉，编写数据处理/分析流程及必要的手段
* 3、增强深度学习的系统性学习（轮廓）
* 4、使用Python的web框架Django，来构造一个真正的web项目（现成的），另外加深对Django特性的了解
* 5、在数据处理方面一定要有亮点如对CNN的理解和实践、对SVM的了解和实战项目
* 6、_不可三心二意，一心一意铺在数据分析和Python上_
* 7、最后分门别类编写对应的心得和技术亮点（新点）
* 8、加强对英语的学习，提升听说能力（important）
* 9、基础的数据结构回顾，必要时可刷leatcode题库

### BAT大数据题库刷记录
* [学习地址](https://blog.csdn.net/sinat_35512245/article/details/78796328)
* 1、请简要介绍SVM
  - SVM，全称support vector machine, 中文称之为支撑向量机。是一种面向数据的分类算法，它的目标是为了确定一个分类超平面，从而将数据分开。
  - 扩展：SVM从简单到繁琐：线性可分SVM（硬隔离）、线性支持向量机（软隔离）、非线性SVM（核函数）。PS：了解什么是硬间隔、软间隔

* 2、请简要介绍Tensorflow的计算图
  - Tensorflow是一个通过计算图的形式来表述计算的编程过程，计算图也叫数据流图，可以把计算图看成一种有向图，Tensorflow中的每一个计算都是计算图上的节点，而节点之间的边描述计算之间的依赖关系。
  - Tensorflow的“计算图”分为2部分
    - 构造部分，包含计算流图
    - 执行部分，通过session来执行图中的计算
* 3、请问GBDT和XGBoost的区别是什么？
  - PS：我不知道这是做什么的-_-[学习地址](https://xijunlee.github.io/2017/06/03/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/)
* 4、在K-means或KNN，我们使用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？
  - 曼哈顿距离只计算水平或垂直距离，有维度限制。另一方面，欧氏距离可用于任何空间的距离计算。因为数据可以存在于任何空间，欧氏距离是更行的选择。例如：国际象棋棋盘使用曼哈顿距离计算，因为它们的在各自的水平和垂直方向做的运动。
* 5、百度2015机器学习笔试题[链接](http://www.itmian4.com/thread-7042-1-1.html)
* 6、简单说说特征工程。
  - 互联网公司的数据挖掘工程师工作内容？
     - 研究各种算法，设计高大上模型...
     - 深度学习的应用，N层神经网络...
  - 大部分复杂模型的算法都是数据科学家在做，大部分同学都是在跑数据、各种map-reduce、hive SQL、数据仓库搬砖、数据清洗、分析业务、找特征LR打天下
    - 关于LR（逻辑回归）。把LR从头到脚讲一遍。建模，现场数学推导，每种解法原理，正则化、LR和maxent模型之间的关系，LR为啥比线性回归好？[学习链接](http://blog.csdn.net/sinat_35512245/article/details/54881672)
* 7、overfitting怎么解决？
  - dropout、正则化（regularization）、batch normalization
* 8、 LR和SVM的关系？[学习链接](http://blog.csdn.net/timcompp/article/details/62237986)
  - 关系：
     - 1、LR和SVM都是用来处理分类问题，一般都用来处理线性二分问题
     - 2、两个方法都可以增加不同的正则化项，如L1、L2等等。所以实验中，两种算法结果很接近
  - 区别：
     - 1、LR是参数模型，SVM是非参数模型。
     - 2、从目标函数来看，区别在于LR采用logistical loss，SVM采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
     - 3、SVM的处理方法是只考虑support vectors，LR通过非线性映射，大大减少了离分类平面远的点的权重，提升了与分类最相关的数据点的权重。
     - 4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
     - 5、logic能做的SVM能做，但在准确率上，SVM可以做logic不能做的。
* 9、LR和线性回归的区别与联系？
  - 1、二者首先都是广义的线性回归
  - 2、经典线性模型的优化目标函数是最小二乘，LR则是似然函数
  - 3、另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 
  - 4、逻辑回归本质是一个线性回归模型，逻辑回归是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。
* 10、谈谈判别式模型和生成式模型？
  - 判别方法：由数据直接学习决策函数 Y = f(X)，或者由条件分布概率P(Y|X)作为预测模型，即判别模型。
  - 生成方法：由数据学习联合概率密度分布函数P(Y|X)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。
  - 由生成模型可以得到判别模型，但由判别模型得不到生成模型。
  - 常见的判别模型：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boostling、条件随机场。
  - 常见的生成模型：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制波尔兹曼机。
* 11、L1和L2的区别
  - L1范数（L1 norm）是指向量中各元素绝对值之和，也有个美称“稀疏规则算子”。
  - L2范数为一个向量中各个元素平方和的1/2次方，L2范数又被称为Euclidean范数或Frobenius范数
  - Lp范数为X向量中各元素绝对值p次方和的1/p次方。
  - 在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 
  - L1范数可以使权值稀疏，方便特征提取。 
  - L2范数可以防止过拟合，提升模型的泛化能力。
* 12、L1和L2正则先验分别服从什么分布？
  - L1服从拉普拉斯分布，L2服从高斯分布
* 13、CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？
  - [答案地址](https://zhuanlan.zhihu.com/p/25005808)
* 14、为什么朴素贝叶斯如此“朴素”？
  - 因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中很不真实，因此，说朴素贝叶斯真的很“朴素”。
* 15、Machine Learning中为何经常对数据做归一化？
  - 1）归一化后加快梯度下降求最优解的速度；
  - 2）归一化后有可能提高精度。[学习链接](http://www.cnblogs.com/LBSer/p/4440590.html)
* 16、谈谈深度学习中的归一化问题？
[答案参考地址](http://www.julyedu.com/video/play/69/686)
* 17、简要说说一个完整ML项目流程。
  - 1）抽象成数学问题
    - 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的工作，胡乱尝试成本很高。这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题。
  - 2）获取数据
    - 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限；数据要有代表性，否则必然会过拟合。
    - 而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
    - 而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。
  - 3）特征预处理与特征选择
    - 良好的数据只有能够提取出良好的特征才能真正发挥效力
    - 特征预处理、数据清洗是关键的步骤，往往能够使得算法的效果和性能得到明星提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们身上。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
    - 筛选出显著特征、摒弃非显性特征，需要ML工程师反复理解业务。这对很多结果有决定性的影响。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
  - 4）训练模型与调优
    - 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优秀。这需要我们对算法的原理有深入的理解。理解越深，就越能发现问题的症结，提出良好的调优方案。
  - 5）模型诊断
    - 如何确定模型调优的方向与思路？这需要对模型进行诊断的技术。
    - 过拟合、欠拟合判断是模型诊断中至关重要的的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
    - 误差分析也是ML至关重要的步骤。通过观察误差样本，全面分析误差产生的原因：是参数问题还是算法选择问题，是特征的问题还是数据本身的问题...
    - 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试，进而达到最优状态。
  - 6）模型融合
    - 一般来说，模型融合能使得效果有一定提升，而且效果很好。
    - 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
  - 7）上线运行
    - 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线运行的效果直接决定模型的成败。不单纯包括其准确程度、误差等情况，还包括其运行的速度（时间复杂度）、资源消耗程度（空间复杂度）、稳定是否可接受。
  - 这些流程主要是工程实践上总结出的一些经验，并不是每个项目都包含完整的一个流程。
* 18、new和malloc的区别？
  - malloc()函数全称memory allocation，中文叫动态内存分配。[学习链接](https://www.cnblogs.com/fly1988happy/archive/2012/04/26/2470542.html)
* 19、hash冲突及解决办法？
  - 1）开放定址法
  - 2）再哈希法；同时构造多个不同的哈希函数
  - 3）链地址法：适合于经常进行插入和删除的情况
  - 4）建立公共溢出区：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表
* 20、如何解决梯度消失和梯度膨胀？
  - （1）梯度消失： 
    - 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。 
    - 可以采用ReLU激活函数有效的解决梯度消失的情况。 
  - （2）梯度膨胀： 
    - 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。
* 21、简单说下有监督学习和无监督学习的区别？
  - 有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR、SVM、BP、RF、GBDT）
  - 无监督学习：对未标记的样本进行训练学习，比如发现这样样本中的结构知识。（K-means,DL）
* 22、了解正则化？
  - 正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。
  - 奥卡姆剃刀原理，能很好的解释已知数据并且十分简单才是最好的模型。
* 23、协方差和相关性有什么区别？
  - 相关性是协方差的标准格式。协方差本身很难比较。因为不同的变量有不同的度量方式，我们通过计算相关性来得到一个介于-1和1之间的值，这就忽略它们各自不同的度量。
* 24、线性分类器与非线性分类器的区别以及优劣？
  - 若模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。
  - 常见线性分类器：LR、贝叶斯分类、单层感知机、线性回归。
  - 常见的非线性分类器：决策树、RF、GBDT、多层感知机。
  - SVM两种都有（线性核还是非线性核：如高斯核）
  - 线性分类器速度快、编程方便，但可能拟合效果不好。
  - 非线性分类器编程复杂，但效果拟合能力强。
* 25、数据的逻辑存储结构（如数组、队列、树、栈等）对于软件开发具有十分重要的影响，试从运行速度、存储效率、适用场景简要分析？
* 26、什么是分布式数据库？
  - 是在集中式数据库系统成熟技术的基础上发展起来的，但不是简单地把集中式数据库分散地实现，它具有自己的性质和特征。集中式数据库系统的许多概念和技术，如数据独立性、数据共享和减少冗余度、并发控制、完整性、安全性和恢复等在分布式数据库系统中都有不同的、更加丰富的内容。
* 27\简单说说贝叶斯定理？
  - 条件概率（后验概率）就是事件A在另一个事件B已经发生条件下的发生概率。P(A|B), P(A|B)=P(A交B)/P(B)
  - 联合概率：两个事件同时发生的概率。
  - 边缘（际）概率：（又称先验概率）：A事件的边缘概率表示为P(A)
  - 考虑一个问题：P(A|B)是在B发生情况下A发生的可能行？
     - P(A|B) = P(B|A)P(A) / P(B)
* 28、简单说下sigmoid激活函数
  - 常用的非线性激活函数有sigmoid、tanh、relu等等，前两者比较常见于全连接层，后者常见于卷积层。
* 29、什么是卷积？
  - 对于图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐各元素相乘在求和）的操作就是所谓的「卷积」操作，也是卷积神经网络的名字来源。
* 30、什么是CNN的池化pool层？
  - 池化，简而言之，即取平均或最大，如：
  -   [1 1 2 4]  --> [6 8]
  -   [5 6 7 8]
* 31、简述什么是生成对抗网络？
  - GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator（生成模型），它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator（判断模型），其目标是判断输入图片是否属于真实训练样本。
* 32、哪些机器算法不需要做归一化处理？
  - 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、GBDT、XGBoost、SVM、LR、KNN、KMeans之类的最优问题需要归一化。
* 33、说说梯度下降
  - [学习地址](http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html)
* 34、梯度下降法找到的一定是下降最快的方向？
  - 并不一定是下降最快的方向，它只是目标函数在当前的点的切平面上下降最快的方向。
* 35、说说随机梯度下降法的问题和挑战？
  - 困难：
    - 1）梯度的计算
    - 2）学习率的选择，学习率太小，导致算法收敛太慢，学习率选择过大容易导致算法不收敛。
* 36、什么是最小二乘法？
  - 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。
* 37、说说Python到底是什么样的语言？[答案解析](http://nooverfit.com/wp/15%E4%B8%AA%E9%87%8D%E8%A6%81python%E9%9D%A2%E8%AF%95%E9%A2%98-%E6%B5%8B%E6%B5%8B%E4%BD%A0%E9%80%82%E4%B8%8D%E9%80%82%E5%90%88%E5%81%9Apython%EF%BC%9F/)
  - Python是解释型语言，Python运行前不需要编译。
  - Python是动态的，不需要在声明变量时指定类型。
  - 面向对象，允许定义类并可以继承和组合，Python没有访问标识符，如C++中的private、public
  - Python中函数是一等公民，它们可以被直接赋值，从其他函数返回值，并传递函数对象，类不是一等公民。
  - Python代码很快，但跑起来会比编译型语言慢，但Python允许使用C扩展写程序，所以瓶颈可以处理。
  - Python的应用场景很多，web开发，自动化，科学建模，大数据应用等。它也经常被视为胶水语言。
  - Python可以简化工作，使得程序员能过关心如何重写代码而不是详细的看一遍底层实现
* 38、Python和多线程。是不是个好主意？列举你觉得可以让Python代码并行运行的方法？
  - Python实际上不允许多线程。它有一个threading包但是如果你想加快代码运行速度，或者想并行运行，这不是一个好主意。Python有一个机制叫全局解释器锁（GIL）。GIL保证每次只有一个线程在解释器中跑。一个线程获得GIL，之后再交给下一个线程。所以，看起来是多个线程在同时跑，但是实际上每个时刻只有CPU核在跑一个线程，没有真正利用多个CPU核跑多个线程。就是说，多个线程在抢着跑一个CPU核。
  - 但是还是有使用threading包的情况的。比如你真的想跑一个可以线程间抢占的程序，看起来是并行的。或者有很长时间的IO等待线程，这个包就会有用。但是threading包不会使用多核去跑代码。
  - 真正的多核多线程可以通过多进程，一些外部库如Spark和Hadoop，或者用Python代码去调用C方法等等来实现。
* 39、怎么对你的代码进行跟踪、协同写代码？
  - 版本号控制！只是关键，你应该很高兴地告诉他们你怎么使用git的，当然svn也是。
* 40、 修饰器的概念？
* 41、Python的垃圾回收机制？
  - Python在内存中维护对象的引用次数。如果一个对象的引用次数变为0，垃圾回收机制会回收这个对象作为他用。
  - 有时候会有“引用循环”的事情发生。垃圾回收器定期检查回收内存。一个例子是，如果你有两个对象 o1 和 o2，并且o1.x == o2 and o2.x == o1. 如果 o1 和 o2 都没有被其他对象使用，那么它们都不应该存在。但是它们的应用次数都是1，垃圾回收不会起作用。
  - 一些启发算法可以用来加速垃圾回收。比如，最近创建的对象更可能是无用的。用创建时间来度量对象的生命时长，生命越长，越可能是更有用的对象。
* 42、其他Python面试题
  - [学习连接](http://www.cnblogs.com/tom-gao/p/6645859.html)
* 43、请写出一段Python代码实现删除一个list里面的重复元素。
  - 要求：使用set函数，set(list);使用字典函数；
  
```python
a = [1,2,4,2,4,5,6,5,7,8,9,0]
b = {}
b = b.fromkeys(a)
c = list(b.keys())
c
```

* 44、编程用sort进行排序，然后从最后一个元素开始判断。
  
```python
a = [1,2,4,2,4,5,7,10,5,5,7,8,9,0,3]
a.sort()
last=a[-1]
for i in range(len(a)-2,-1,-1):
    if last==a[i]:
        del a[i]
    else: last=a[i]
    print(a)
```

* 45、Python里面如何生成随机数？
  - random模块
    - 随机整数：random.randint(a,b):返回随机整数x, a<=x<=b
    - random.randrange(start,stop,[step])
    - 随机实数：random.random()：返回（0，1）之间的浮点数
    - random.uniform(a,b)：返回指定范围内的浮点数
* 46、说说常见的损失函数
  - 对于给定的输入X,由f(X)输出相应的Y，这个输出的预测值f(X)与真实的Y可能一致，也可能不一致（误差在所难免），用一个损失函数来度量预测错误的程度，这个损失函数记住L(Y, f(x))
  - 常用的损失函数有：
    - 0-1损失函数
    - 平方损失函数
    - 对数损失函数
* 47、简单介绍逻辑回归LR
  - 目的：从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此使用LR函数（或称为sigmoid函数）将自变量映射到（0，1）上，映射后的值被认为是属于y=1的概率。
* 48、以下哪些方法不可以直接对文本分类
  - 决策树、SVM、KNN都可以直接对文本分类；但Kmeans不行，Kmeans是聚类方法，无监督学习
* 49、Kmeans的复杂度？
  - 时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数
  - 空间复杂度：O((m+K)n)
* 50、影响聚类算法结果的主要因素有
  - 分类标准、特征选择、模式相似性测度
* 51、
  - （1）模式识别中，马氏距离较之欧氏距离的优点：尺度不变形、考虑模式的分布
  - （2）影响基本K均值算法的主要因素：样本输入顺序、模式相似性测度、聚类准则
  - （3）在统计模式分类问题中，当先验概率未知时，可以使用 「最小最大损失准则」、「N-P判决」
  - （4）欧氏距离具有「平移不变性」「旋转不变性」
  - （5）马氏距离具有「尺度缩放不变性」「不受量纲影响的特性」「平移不变性」「旋转不变性」
* 52、你有哪些DL（RNN、CNN）调参经验？
  - [学习地址](https://www.zhihu.com/question/41631631)
* 53、说说RNN（循环神经网络）原理
  - [相关题目1](http://blog.csdn.net/heyongluoyao8/article/details/48636251)
  - [相关题目2](https://zhuanlan.zhihu.com/p/28054589)
* 54、当机器学习性能遭遇瓶颈时，如何优化？
  - 可以从4个方面进行尝试：基于数据、借助算法、用算法调参、借助模型融合。谈多谈少看经验和心得。
  - [机器学习改善备忘录](http://blog.csdn.net/han_xiaoyang/article/details/53453145)
* 55、做过什么样的机器学习项目？比如如何从零构建一个推荐系统？
  - 看个人项目了，所以说还是要有实战的经验。「数据分析项目」
* 56、数据预处理
  - （1）缺失值，填充缺失值fillna：
    - 1）离散：None
    - 2）连续：均值
    - 3）缺失值太多，则直接去除该列
  - （2）连续值：离散化。有的模型（如决策树）需要离散化
  - （3）对定量特征二值化。核心在于设定一个阈值，大于阈值设置为1，小于等于阈值赋值为0.
  - （4）皮尔逊相关系数，去除高度相关的列
* 57、如何进行特征选择？
  - 特征选择是一个重要的数据预处理过程，主要有两个原因：
    - 一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;
    - 二是增强对特征和特征值之间的理解。
  - 常用的特征选择方式：
    - 去除方差较小的特征
    - 正则化。L1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应的系数飞零。
    - 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
    - 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。
* 58、你知道有哪些数据处理和特征工程的处理？
![特征工程](https://img-blog.csdn.net/20171214121831941?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* 59、SVD和PCA
  - PCA的理念是使得数据投影后的方差最大，找到这样一个投影，满足方差最大的条件即可。而经过去除均值的操作后，就可以使用SVD分解来求解这样的一个投影向量，选择特征最大的方向。
* 60、数据不平衡问题？
  - 这主要是由数据的分布不平衡造成的，解决方案如下：
    - 1）采样，对小样本加噪声采样，对大样本进行下采样 
    - 2）进行特殊的加权，如在Adaboost中或者SVM中 
    - 3）采用对不平衡数据集不敏感的算法 
    - 4）改变评价标准：用AUC/ROC来进行评价 
    - 5）采用Bagging/Boosting/Ensemble等方法 
    - 6）考虑数据的先验分布
* 61、简述神经网络的发展
  - MP模型+sgn——>单层感知机（只能线性）+sgn——>Minsky 低谷 ——>多层感知机+BP+Sigmoid——>(低谷) ——>深度学习+Pretraining+ReLU/Sigmoid
* 62、深度学习常用方法
  - [参考地址](http://blog.csdn.net/u010496169/article/details/73550487)
* 63、卷积神经网络可以对一个输入进行多种变换（旋转、平移、缩放），这表示正确吗？
  - 把数据传入神经网络之前需要做一系列的数据预处理（也就是旋转、平移、缩放）工作，神经网络本身不能完成这些变换。
* 64、常见的监督学习算法有哪些？
  - 感知机、SVM、人工神经网络、决策树、逻辑回归
* 65、数据清理中，处理缺失值的方法？
  - 估算、整例删除、变量删除、成对删除
* 66、试推导样本空间中任意点x到超平面（w，b）的距离公式
![超平面距离](https://img-blog.csdn.net/20171214141457046?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* 67、CNN常用的几个模型
![CNN模型](https://img-blog.csdn.net/20171214141903341?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMzU1MTIyNDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
* 68、带核函数的SVM为什么能分类非线性问题？
  - 核函数的本质就是两个函数的内积，而这个函数在svm中可以表示成对输入值的高维映射。注意核并不是直接对应映射，核只不过是一个内积。（感觉答非所问）
* 69、常用核函数即核函数条件
  - 核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。 
RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。 
线性核：主要用于线性可分的情况 
多项式核
* 70、逻辑回归相关问题
  - 公式一定会推
  - LR基本概念，最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。
  - L1、L2范式
  - LR与SVM的比较
  - LR与随机森林的比较
  - 常用优化方法
* 71、什么是共线性，跟过拟合有什么联系？
  - 共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。 
共线性会造成冗余，导致过拟合。
  - 解决方法：排除变量的相关性或加入权重正则。
* 72、机器学习中的正负样本
  - 在分类问题中，这个问题相对好理解一点，比如人脸识别中的例子，正样本很好理解，就是人脸的图片，负样本的选取就与问题场景相关，具体而言，如果你要进行教室中学生的人脸识别，那么负样本就是教室的窗子、墙等等，也就是说，不能是与你要研究的问题毫不相关的乱七八糟的场景图片，这样的负样本并没有意义。负样本可以根据背景生成，有时候不需要寻找额外的负样本。一般3000-10000的正样本需要5，000,000-100,000,000的负样本来学习，在互金领域一般在入模前将正负比例通过采样的方法调整到3:1-5:1。
* 73、对于维度极低的特征，选择线性还是非线性分类器？
  - 非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。
    - 1.如果特征的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM。 
    - 2.如果特征的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel。 
    - 3.如果特征的数量比较小，而样本数量很多，需要手工添加一些特征变成第一种情况。
* 74、SVM、LR和决策树的对比
  - 模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;决策树容易过拟合，需要进行剪枝。 
  - 损失函数：SVM hinge loss; LR L2正则化; Adaboost 指数损失。 
  - 数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感。 
  - 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核。
* 75、简述KNN最近邻分类算法的过程？
  - 1计算训练样本和测试样本中每个样本点的距离（常用距离欧氏距离、马氏距离）
  - 2对上面所有的距离进行排序；
  - 3选前K个最小距离的样本；
  - 4根据这K个样本的标签进行投票，得到最后的分类类别。
* 76、特征向量的归一化方法有哪些？
  - 线性函数转换
  - 对数函数转换
  - 反余切函数转换
  - 减去均值，除以方差
* 77、优化算法及其优缺点？
  - 温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。 
  - 1）随机梯度下降：优点：可以一定程度上解决局部最优解的问题  ；   缺点：收敛速度较慢
  - 2）批量梯度下降：优点：容易陷入局部最优解   ；  缺点：收敛速度较快 
  - 3）mini_batch梯度下降：综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
  - 4）牛顿法：牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。
  - 5）拟牛顿法：拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

> 本文首发[YunLong-Blog](https://ZhaoYLong.github.io)，题目摘自CSDN、博客园等博客平台。
